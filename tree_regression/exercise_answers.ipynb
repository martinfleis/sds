{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ff264d-065c-4509-ba2f-719397cba0b3",
   "metadata": {},
   "source": [
    "# Hedonic prices in Busan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef37192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import esda\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyinterpolate\n",
    "from libpysal import graph\n",
    "from sklearn import ensemble, metrics, model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import contextily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef704f",
   "metadata": {},
   "source": [
    "### 1. Load the data and convert it to a geodataframe, make sure you have the correct local projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc00ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "url = \"https://martinfleischmann.net/sds/tree_regression/data/prices_data.csv\"\n",
    "prices = pd.read_csv(url)\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f7a77-3611-415b-85ea-3ddb53d008b5",
   "metadata": {},
   "source": [
    "The `prices` DataFrame contains columns with Latitude and Longitude, but no geometry. First we need to create a GeoDataFrame from the `prices` DataFrame. The `geometry` column should contain the Point objects created from the Latitude and Longitude columns. Be careful to assign correct local coordinate system to the GeoDataFrame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3861875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read as geodataframe\n",
    "prices = gpd.GeoDataFrame(\n",
    "    prices,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        prices[\"Longitude\"], prices[\"Latitude\"], crs=\"EPSG:4326\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc90d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate and assign UTM CRS\n",
    "crs_prices = prices.estimate_utm_crs()\n",
    "prices = prices.to_crs(crs_prices)\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336034c",
   "metadata": {},
   "source": [
    "### 2. The target variable is ‘Property Prices’. Choose at least five independent variables that you think will be good predictors of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the target variable\n",
    "ax = prices.plot(\n",
    "    'Property Prices',\n",
    "    legend=True,\n",
    "    vmin=prices['Property Prices'].min(),\n",
    "    vmax=prices['Property Prices'].max(),\n",
    "    cmap='magma_r',\n",
    "    markersize=5,\n",
    ")\n",
    "contextily.add_basemap(ax=ax, crs=prices.crs, source=\"CartoDB Positron\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b642a790",
   "metadata": {},
   "source": [
    "You have your target variable. The models will try to predict it based\n",
    "on a set of independent variables that you will choose. Let's pick the\n",
    "following independent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9502dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_variables = ['Size', 'Floor', 'Units', 'Parking', 'Dist. Green', 'Heating', 'Dist. Subway', 'Bus Stop', 'High School']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a528250",
   "metadata": {},
   "source": [
    "You can briefly check how each looks on a map and compare them to the\n",
    "distribution of prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(10, 10))\n",
    "for variable, ax in zip(independent_variables, axs.flatten()):\n",
    "    prices.plot(\n",
    "        variable,\n",
    "        ax=ax,\n",
    "        cmap=\"magma_r\",\n",
    "        legend=True,\n",
    "        markersize=0.5\n",
    "    )\n",
    "    ax.set_title(variable, fontdict={\"fontsize\": 8})\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fde7d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the model\n",
    "independent = prices[independent_variables]\n",
    "target = prices['Property Prices']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848868f0",
   "metadata": {},
   "source": [
    "### 3. Prepare you data for modeling - split it into training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf624cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    independent, target, test_size=0.2, random_state=0\n",
    ")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc98786e-a03c-4c8b-a20b-575c675eeb72",
   "metadata": {},
   "source": [
    "You can check that `X_*`, containing independent variables is split into\n",
    "two parts, one with 80% of the data and the other with remaining 20%.\n",
    "Completely randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed0ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee94df8a-0e07-468e-9d42-b4a66c0ae169",
   "metadata": {},
   "source": [
    "### 4. Try to fit a linear regression model and compare it with a tree based model of your choice, is there a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression(n_jobs=-1)\n",
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06e4b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_lin = linear_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the evaluation metrics\n",
    "r2 = metrics.r2_score(y_test, pred_test_lin)\n",
    "mean_absolute_error = metrics.mean_absolute_error(y_test, pred_test_lin)\n",
    "rmse = metrics.root_mean_squared_error(y_test, pred_test_lin)\n",
    "\n",
    "# Print the summary\n",
    "summary = f\"\"\"\\\n",
    "Evaluation metrics\n",
    "==================\n",
    "Linear regression:\n",
    "  R2:   {round(r2, 3)}\n",
    "  MAE:  {round(mean_absolute_error, 3)}\n",
    "  RMSE: {round(rmse, 3)}\n",
    "\"\"\"\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06670fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the random forest model\n",
    "rf_model = ensemble.RandomForestRegressor(n_jobs=-1, random_state=0)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f368d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set\n",
    "pred_test_rf = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea88cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the evaluation metrics\n",
    "r2 = metrics.r2_score(y_test, pred_test_rf)\n",
    "mean_absolute_error = metrics.mean_absolute_error(y_test, pred_test_rf)\n",
    "rmse = metrics.root_mean_squared_error(y_test, pred_test_rf)\n",
    "\n",
    "# Print the summary\n",
    "summary = f\"\"\"\\\n",
    "Evaluation metrics\n",
    "==================\n",
    "Random Forest:\n",
    "  R2:   {round(r2, 3)}\n",
    "  MAE:  {round(mean_absolute_error, 3)}\n",
    "  RMSE: {round(rmse, 3)}\n",
    "\"\"\"\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871ffa3",
   "metadata": {},
   "source": [
    "You can see that the random forest model performs better. R2 is higher by more than 10% and both RMSE and MAE are lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b2adb",
   "metadata": {},
   "source": [
    "### 5. Cross validate your model and evaluate its performance. Plot the residuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cross_val = model_selection.cross_val_predict(\n",
    "    rf_model,\n",
    "    independent,\n",
    "    target,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "prices.plot(pred_cross_val, legend=True, markersize = 5, cmap='magma_r').set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd3a23e-9a63-4c53-8780-6b33079bfc40",
   "metadata": {},
   "source": [
    "Cross-validation also allows you to assess the quality of the model more\n",
    "reliably, minimising the effect of sampling on the metric. You can\n",
    "simply measure the performance on the full array taking into account\n",
    "every of the five folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_cross_val = metrics.r2_score(\n",
    "    target, pred_cross_val\n",
    ")\n",
    "mae_cross_val = metrics.mean_absolute_error(\n",
    "    target, pred_cross_val\n",
    ")\n",
    "rmse_cross_val = metrics.root_mean_squared_error(\n",
    "    target, pred_cross_val\n",
    ")\n",
    "\n",
    "summary += f\"\"\"\\\n",
    "Random Forest (k-fold metrics):\n",
    "  R2:   {round(r2_cross_val, 3)}\n",
    "  MAE:  {round(mae_cross_val, 3)}\n",
    "  RMSE: {round(rmse_cross_val, 3)}\n",
    "\"\"\"\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dee7d0a-c190-4386-87ea-f66da9aed592",
   "metadata": {},
   "source": [
    "These results are not wildly off but the performance dropped a bit. The\n",
    "smaller the dataset (and this one is pretty small) the higher the effect\n",
    "of train-test split could be. Let’s refer to the cross-validated metrics\n",
    "as more reliable representation of the performance of the baseline model\n",
    "here.\n",
    "\n",
    "#### Residuals\n",
    "\n",
    "Plot the residuals of the model to see if there is any pattern in\n",
    "the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad99bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = (target - pred_cross_val)\n",
    "\n",
    "minmax = residuals.abs().std()\n",
    "prices.plot(\n",
    "    residuals,\n",
    "    vmin=-minmax * 5,\n",
    "    vmax=minmax * 5,\n",
    "    cmap=\"RdBu\",\n",
    "    legend=True,\n",
    "    markersize=2\n",
    ").set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca3a9fa-2b90-4a36-96c7-be05221d0158",
   "metadata": {},
   "source": [
    "The over-predicted values are more common in the city center. The\n",
    "under-predicted values are more common in the outskirts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0e6c6",
   "metadata": {},
   "source": [
    "### 6. Perform spatial evaluation using the \"hex_id\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "681aeb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices[\"prediction\"] = pred_cross_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd1a97-45be-4753-97cf-bc83547d89ea",
   "metadata": {},
   "source": [
    "Using `groupby`, you can group the data by `\"hex_id\"` and check the\n",
    "metric within each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6504517",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = prices.groupby(\"hex_id\")[\n",
    "    [\"Property Prices\", \"prediction\"]\n",
    "]\n",
    "\n",
    "\n",
    "block_mae = grouped.apply(\n",
    "    lambda group: metrics.mean_absolute_error(\n",
    "        group[\"Property Prices\"], group[\"prediction\"]\n",
    "    )\n",
    ")\n",
    "block_rmse = grouped.apply(\n",
    "    lambda group: metrics.root_mean_squared_error(\n",
    "        group[\"Property Prices\"], group[\"prediction\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c711c1-c9ad-453c-94ae-a0aabbce19f1",
   "metadata": {},
   "source": [
    "Let’s concatenate them together to a single DataFrame with proper column\n",
    "names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5d7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_metrics = pd.concat([block_mae, block_rmse], axis=1)\n",
    "spatial_metrics.columns = [\"block_mae\", \"block_rmse\"]\n",
    "spatial_metrics.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f2e28",
   "metadata": {},
   "source": [
    "And merge with the original data. The spatial metrics cannot be simply\n",
    "assigned as new columns as they are much shorter - only one value per\n",
    "*hex_id*. You need to merge on the *hex_id* values to assign it as new\n",
    "columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50840c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = prices.merge(\n",
    "    spatial_metrics, left_on=\"hex_id\", right_index=True\n",
    ")\n",
    "prices.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c84a06",
   "metadata": {},
   "source": [
    "### 7. Compare blocked spatial metrics based on \"hex_id\" with the smoothed spatial metrics using graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dc60f8-4d07-437b-8bc0-2f07778e76e6",
   "metadata": {},
   "source": [
    "Let’s see how the performance differs across space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df35bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "for i, metric in enumerate([\"block_mae\", \"block_rmse\"]):\n",
    "    prices.plot(metric, ax=axs[i], legend=True, cmap=\"cividis\", markersize=2)\n",
    "    axs[i].set_title(metric, fontdict={\"fontsize\": 8})\n",
    "    axs[i].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340ecb89-1c3e-4d14-93be-d9eb3d6935e4",
   "metadata": {},
   "source": [
    "The spatial variation is evident. The model performs better in some\n",
    "areas and worse in others. \n",
    "\n",
    "Another option is to measure the spatial metrics using the `Graph`.\n",
    "In this case, you can assume that the price of a property in a given location\n",
    "is influenced by the prices in neighbouring locations. The\n",
    "so let’s get neighbors in the 1000m radius.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1f94022",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_band = graph.Graph.build_distance_band(\n",
    "    prices, 1000\n",
    ").assign_self_weight()\n",
    "\n",
    "prices[\"spatial_mae\"] = distance_band.apply(\n",
    "    prices[[\"Property Prices\", \"prediction\"]],\n",
    "    lambda df: metrics.mean_absolute_error(\n",
    "        df[\"Property Prices\"], df[\"prediction\"]\n",
    "    ),\n",
    ")\n",
    "prices[\"spatial_rmse\"] = distance_band.apply(\n",
    "    prices[[\"Property Prices\", \"prediction\"]],\n",
    "    lambda df: metrics.root_mean_squared_error(\n",
    "        df[\"Property Prices\"], df[\"prediction\"]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3a410-4750-4048-80b5-b6810f7cfcf1",
   "metadata": {},
   "source": [
    "You can map the results again, observing much smoother transitions\n",
    "between low and high values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1414cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "for i, metric in enumerate([\"spatial_mae\", \"spatial_rmse\"]):\n",
    "    prices.plot(metric, ax=axs[i], legend=True, cmap=\"cividis\", markersize=2)\n",
    "    axs[i].set_title(metric, fontdict={\"fontsize\": 8})\n",
    "    axs[i].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20aab20",
   "metadata": {},
   "source": [
    "### 8. Check the spread of the asummed auotocorrelation using variogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8561114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.hstack(\n",
    "    [\n",
    "        prices.get_coordinates(),\n",
    "        residuals.abs().values.reshape(-1, 1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80fca85-7878-42ff-b3f4-5afbbcc5b147",
   "metadata": {},
   "source": [
    "Build the variogram, ideally covering the width of the whole city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_semivar = pyinterpolate.build_experimental_variogram(\n",
    "    input_array=input_data,\n",
    "    step_size=500,\n",
    "    max_range=20000,\n",
    ")\n",
    "exp_semivar.plot(plot_covariance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2560ae9-ceb7-4e47-b732-103167962d5d",
   "metadata": {},
   "source": [
    "The semivariance tends to grow nearly across the whole range, indicating\n",
    "that the autocorrelation does not disappear when considering larger\n",
    "distances. The results clearly suggests\n",
    "that the model has a troubles with spatial heterogeneity of the\n",
    "relationship between independent variables and the target one.\n",
    "\n",
    "### 9. Perform LISA on the residuals - locate underpriced and overpriced properties, and clusters of consistently correct and consistently wrong locations.\n",
    "\n",
    "One approach of determining spatial dependence of the residuals you are\n",
    "already familiar with is measuring local indicators of spatial\n",
    "autocorrelation. The variogram does not really help us in selecting the\n",
    "optimal neighborhood to work with, so let’s build a distance band graph\n",
    "with the threshold of 1 km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "790e4bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_1km = graph.Graph.build_distance_band(\n",
    "    prices, 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under-predicted and over-predicted\n",
    "moran = esda.Moran_Local(residuals, distance_1km, permutations=9999, seed=0)\n",
    "moran.explore(prices, tiles = \"CartoDB dark_matter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f835e",
   "metadata": {},
   "source": [
    "High-High clusters are those that are consistently under-predicted while Low-Low are those consistently over-predicted based on the spatial distribution of residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcda104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters of consistently correct and consistently wrong locations.\n",
    "moran_abs = esda.Moran_Local(residuals.abs(), distance_1km, permutations=9999, seed=0)\n",
    "moran_abs.explore(prices, tiles=\"CartoDB dark_matter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4d2a77-ba42-4674-b61f-ee735df97416",
   "metadata": {},
   "source": [
    "This time, High-High captures clusters of high error rates, while\n",
    "Low-Low areas of low error rate.\n",
    "\n",
    "### 10. Build a model which avoids spatial leakage using the \"hex_id\" and tune its hyperparameters. \n",
    "- Is the performace different from the previous models? \n",
    "- Which features are important in this model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ce9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in independent_variables + [\"Property Prices\"]:\n",
    "    morans_i = esda.Moran(prices[variable], distance_band)\n",
    "    print(f\"Moran's I of {variable} is {morans_i.I:.2f} with the  p-value of {morans_i.p_sim}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de48906",
   "metadata": {},
   "source": [
    "Some of the variables are spatially more or less autocorrelated, which means that\n",
    "there is a risk of spatial leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f5707c-f14f-4526-aa5c-7f85dce34bba",
   "metadata": {},
   "source": [
    "Spatial cross-validation mitigates the issue by including a spatial\n",
    "dimension in the train-test split. The aim is to divide the whole study\n",
    "area into smaller regions and allocate whole regions to train and test\n",
    "splits. \n",
    "\n",
    "Instead of using `KFold`, use `GroupKFold`, which ensures that\n",
    "observations are allocated into splits by groups (all observations\n",
    "within a single group will be in a single split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b95a85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_hex = prices[independent_variables + [\"hex_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = model_selection.GroupKFold(n_splits=5)\n",
    "splits = gkf.split(\n",
    "    independent,\n",
    "    groups=independent_hex[\"hex_id\"]\n",
    ")\n",
    "split_label = np.empty(len(independent), dtype=float)\n",
    "for i, (train, test) in enumerate(splits):\n",
    "    split_label[test] = i\n",
    "\n",
    "prices.plot(\n",
    "    split_label, categorical=True, legend=True, cmap=\"Set3\",markersize=2\n",
    ").set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0166bb4f-b496-42e0-9e9e-a1ebc04ff2fc",
   "metadata": {},
   "source": [
    "Cross-validated prediction can then be performed using these splits,\n",
    "ensuring that the spatial leakage between test and train is limited in\n",
    "each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5ad4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_spatial_cv = ensemble.RandomForestRegressor(random_state=0, n_jobs=-1)\n",
    "\n",
    "pred_spatial_cv = model_selection.cross_val_predict(\n",
    "    rf_spatial_cv,\n",
    "    independent,\n",
    "    target,\n",
    "    groups=prices[\"hex_id\"],\n",
    "    cv=gkf,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aad25d-dac6-4bea-8ad4-b3ecd57cca5f",
   "metadata": {},
   "source": [
    "The rest can follow the same pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c07a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_spatial_cv = metrics.r2_score(target, pred_spatial_cv)\n",
    "mae_spatial_cv = metrics.mean_absolute_error(target, pred_spatial_cv)\n",
    "rmse_spatial_cv = metrics.root_mean_squared_error(target, pred_spatial_cv)\n",
    "\n",
    "summary += f\"\"\"\\\n",
    "Random Forest with spatial cross-validation (k-fold):\n",
    "  R2:   {round(r2_spatial_cv, 3)}\n",
    "  MAE:  {round(mae_spatial_cv, 3)}\n",
    "  RMSE: {round(rmse_spatial_cv, 3)}\n",
    "\"\"\"\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5beb988-db4b-4f4d-be33-aa0479ea67ee",
   "metadata": {},
   "source": [
    "The models with spatial cross-validation usually show worse performance\n",
    "than those with the random one but that is expected. The difference is\n",
    "due to elimination of the spatial leakage and hence improving the\n",
    "robustness of the model, meaning that on unseen data, it will perform\n",
    "better (contrary to the change in the metrics).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8e1cb-b86f-48d3-98d4-e0bbfbf03008",
   "metadata": {},
   "source": [
    "\n",
    "#### Hyper-parameter tuning\n",
    "\n",
    "When searching for an optimal model, you shall test different\n",
    "hyper-parameters. Let’s stick to the random forest model and try to find\n",
    "the optimal number of trees and the maximum features to consider when\n",
    "looking for the best split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fda05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "}\n",
    "\n",
    "rf = ensemble.RandomForestRegressor(random_state=0)\n",
    "\n",
    "grid_search = model_selection.GridSearchCV(\n",
    "   rf, param_grid, cv=gkf\n",
    ")\n",
    "grid_search.fit(independent, target, groups=prices[\"hex_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d64e10-7803-4d1a-8b43-bfecdb1d6776",
   "metadata": {},
   "source": [
    "\n",
    "Let’s extract the mean test scores per each model and figure out which\n",
    "parameters are the best in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = grid_search.cv_results_[\"params\"]\n",
    "mean_scores = grid_search.cv_results_[\"mean_test_score\"]\n",
    "\n",
    "grid_search_results = pd.DataFrame(params)\n",
    "grid_search_results[\"mean_score\"] = mean_scores\n",
    "grid_search_results.sort_values(\"mean_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4260e931-d117-4016-9517-d525aef003df",
   "metadata": {},
   "source": [
    "The best model seems to be the one with 300 trees and `sqrt` number of\n",
    "features to consider, which is the default value for the random forest.\n",
    "\n",
    "#### Feature importance\n",
    "\n",
    "Get the best model from the grid search and explore the importance of\n",
    "individual independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eef27d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907eb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.Series(\n",
    "    best_model.feature_importances_, index=independent_variables\n",
    ")\n",
    "feature_importance.sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5460b",
   "metadata": {},
   "source": [
    "The most important variable is the `Size` of the property, followed by\n",
    "the `Parking` variable. The `Heating` variable is the least important."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
